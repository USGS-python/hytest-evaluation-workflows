{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e11273a-f755-46a7-994d-7bb4e5367e9e",
   "metadata": {},
   "source": [
    "# HyTest model benchmarking workflow (traditional metrics)\n",
    "**Timothy Hodson and Rich Signell**\n",
    "\n",
    "This notebook demonstrates a computational workflow for benchmarking the National Water Model, and is meant to provide an adaptable template for benchmarking other Earth-system models and datasets. The HyTest benchmark workflow consists of three components:\n",
    "1. a set of model predictions and observations to compare them against.\n",
    "1. the domain over which to compute benchmark\n",
    "1. a set of statistical metrics with which to benchmark\n",
    "\n",
    "The basic workflow loads the predictions and observations, subsets them to the domain of the benchmark, then calculate metrics over that domain.\n",
    "Any benchmark is fully reproducable given a workflow notebook and the correct versions of each of these three components.\n",
    "\n",
    "In practice, the datasets may be too large to fit in memory or to transfer over the internet, so this notebook will demonstrate several open-source Python libraries for 'moving the computations to the data.' Some of these tools are relatively new, but they are quickly becoming standards within the Earth-science community\n",
    "\n",
    "The notebook is organized into a series of helper functions that handle tasks like loading data, configuring compute resources, and computing metrics over a chunk of data. Once these are defined, the analysis can be run in a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32021b3-9141-4acb-9256-b66cf9a08c0d",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "## 0.0. Load libraries\n",
    "Prior to beginning, ensure that following Python librariers are installed and loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7e186-83be-4dae-87d8-d469beb4a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.bag as db\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import intake\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db8121-7e23-4b52-8694-4e9fa1a17478",
   "metadata": {},
   "source": [
    "## 0.1. Configure cluster\n",
    "The notebook shows example configurations that might be used for three different computing resources supported by USGS, including Denali, Tallgrass, and Cloud.\n",
    "\n",
    "First, select the computing resource on which to run your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdcad1-d21d-4340-99ac-9d4ad032685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = 'tallgrass' #denali, tallgrass, local, esip-qhub-gateway-v0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bdb0cd-89ec-4f07-bf68-b60b8e231140",
   "metadata": {},
   "source": [
    "How to configure the cluster will vary among these resources, so we've created a helper function to take care of that.\n",
    "\n",
    "TODO: short explanation of the configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceced23d-31a2-416f-a316-2a3df92e7134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def configure_cluster(resource):\n",
    "    ''' Helper function to configure cluster\n",
    "    '''\n",
    "    if resource == 'denali':\n",
    "        cluster = LocalCluster(threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "    \n",
    "    elif resource == 'tallgrass':\n",
    "        cluster = SLURMCluster(queue='cpu', cores=1, interface='ib0',\n",
    "                               job_extra=['--nodes=1', '--ntasks-per-node=1', '--cpus-per-task=1'],\n",
    "                               memory='6GB')\n",
    "        cluster.adapt(maximum_jobs=30)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif resource == 'local':\n",
    "        import os\n",
    "        import warnings\n",
    "        warnings.warn(\"Running locally can result in costly data transfers!\\n\")\n",
    "        n_cores = os.cpu_count() # set to match your machine\n",
    "        cluster = LocalCluster(threads_per_worker=n_cores)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "    elif cluster_type in ['esip-qhub-gateway-v0.4']:   \n",
    "        import sys, os\n",
    "        sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "        import ebdpy as ebd\n",
    "        ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "        aws_profile = 'esip-qhub'\n",
    "        aws_region = 'us-west-2'\n",
    "        endpoint = f's3.{aws_region}.amazonaws.com'\n",
    "        ebd.set_credentials(profile=aws_profile, region=aws_region, endpoint=endpoint)\n",
    "        worker_max = 30\n",
    "        client,cluster = ebd.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                              region=aws_region, use_existing_cluster=True,\n",
    "                                              adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                              worker_profile='Medium Worker', propagate_env=True)\n",
    "        \n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713623c-d507-4213-b534-5204bd9e56a9",
   "metadata": {},
   "source": [
    "# 1. Define performance benchmark\n",
    "A performance benchmark consists of three components: (1) a set of predictions and observations, (2) the domain over which to benchmark (3) a set of statistical metrics with which to benchmark. The basic workflow is to load the predictions and observations, subset them to the domain of the benchmark, then calculate metrics over that domain.\n",
    "\n",
    "## 1.0 Load data\n",
    "Let's begin by introducing [Intake](https://github.com/intake/intake), which is a set of tools for loading and sharing data in data science projects. Data from this project are stored within an Intake catalog. We can inpsect that catalog with the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7743279-a185-408f-81cf-0aedad65c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/nhm-usgs/data-pipeline-helpers/main/hytest/hytest_intake_catalog.yml'\n",
    "cat = intake.open_catalog(url)\n",
    "print(list(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fd92a-1659-4192-b7b5-367a823f266e",
   "metadata": {},
   "source": [
    "Using the Intake catalog, we define another helper function that loads our data from the appropriate location depending on where the computation will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ddbbe-ebd2-452c-ac86-5fd20831181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_streamflow_data(resource):\n",
    "    ''' Helper function to load observations and model predictions from Intake.\n",
    "    \n",
    "    Some initial preprocessing is also done here, like converting the datasets to the same type.\n",
    "    '''\n",
    "    if resource in ['tallgrass','denali']:\n",
    "        location = 'onprem'\n",
    "        \n",
    "    elif resource in ['esip-qhub-gateway-v0.4']:\n",
    "        location = 'cloud'\n",
    "        \n",
    "    url = 'https://raw.githubusercontent.com/nhm-usgs/data-pipeline-helpers/main/hytest/hytest_intake_catalog.yml'\n",
    "    cat = intake.open_catalog(url)\n",
    "\n",
    "    observations_ds = cat[f'nwis-streamflow-usgs-gages-{location}'].to_dask()\n",
    "    model_ds = cat[f'nwm21-streamflow-usgs-gages-{location}'].to_dask()\n",
    "    \n",
    "    observations = observations_ds['streamflow']\n",
    "    model = model_ds['streamflow'].astype('float32')\n",
    "\n",
    "    observations.name = 'observed'\n",
    "    model.name = 'predicted'\n",
    "    \n",
    "    return observations, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c773a-9ec3-42b5-8d1b-66d0b9735181",
   "metadata": {},
   "source": [
    "Let's demo that helper, and show how to select data for a single streamgage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e48b49-4fe6-4a7c-8754-75d17d86acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, pred = load_streamflow_data(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550137d-e140-4ce7-b101-9eaeaa7cc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# time it takes to read a single gage\n",
    "gage_id = 'USGS-01030350'\n",
    "obs.sel(gage_id=gage_id).load(scheduler='threads').to_series().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8921f-5b2c-43fc-a3d7-98ba79972421",
   "metadata": {},
   "source": [
    "## 1.1 Load benchmark\n",
    "Each benchmark is defined over a specific domain (typically bounded in space and time). Benchmark domains are published to Science Base, or they can be defined within the notebook. \n",
    "\n",
    "TODO: Here we load the the benchmark domain from the cloud, but this will be changed to Science Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bbe17-9e33-405b-96cf-29f180403c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load benchmark sites directly from Science Base\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "url = 's3://esip-qhub-public/usgs/hytest/streamflow_benchmark_sites_v09.csv'\n",
    "benchmark_ds = pd.read_csv(fs.open(url), dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str}).set_index('site_no').to_xarray()\n",
    "\n",
    "# Format the site_no\n",
    "benchmark_ds['site_no'] = [f'USGS-{site}' for site in benchmark_ds['site_no'].values]\n",
    "\n",
    "benchmark_gages = benchmark_ds['site_no'].values.tolist()\n",
    "benchmark_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c78ae4-9da7-4461-aa30-2a2c68e67dc7",
   "metadata": {},
   "source": [
    "## 1.2. Load statistical metrics\n",
    "This demo computes a benchmark for the National Water Model (NWM) using a suite of traditional metrics, which are defined in the following cell. In practice these may be loaded from external libraries. Ideally, the notebook would be configured to load a specific version of that library, which would be used to uniquely identify the benchmark.\n",
    "\n",
    "TODO: in the future, specific metrics packages could be loaded with `httpimport`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62327185-50a2-4fa6-bdd1-5a876c918001",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A selection of traditional statistical metrics\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mse(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error (MSE)\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((obs - mod) ** 2)\n",
    "\n",
    "\n",
    "def nse(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the Nash-Sutcliffe Efficiency (NSE)\n",
    "    (https://www.sciencedirect.com/science/article/pii/0022169470902556?via%3Dihub)\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Nash-Sutcliffe Efficiency\n",
    "    \"\"\"\n",
    "    return 1 - (mse(obs, mod) / np.var(obs))\n",
    "\n",
    "\n",
    "def pbias(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the percent bias\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Percent bias\n",
    "    \"\"\"\n",
    "    return 100 * ((np.sum(mod - obs)) / (np.sum(obs)))\n",
    "\n",
    "\n",
    "def pbias_percentile(obs, model, percentile, fun):\n",
    "    \"\"\"\n",
    "    Calculate the percent bias for a percentile bin\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "        percentile: float\n",
    "        fun: comparison function (e.g., np.greater)\n",
    "    Returns:\n",
    "        Percent bias for bin\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(obs, q=percentile)\n",
    "    i = fun(obs, threshold)\n",
    "    \n",
    "    return pbias(obs[i], model[i])\n",
    "    \n",
    "\n",
    "def pearson_r(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate Pearson's r\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Pearson's r\n",
    "    \"\"\"\n",
    "    #return np.cov(mod, obs) / np.sqrt( np.var(mod) * np.var(obs))\n",
    "    return np.corrcoef(mod, obs)[0,1]\n",
    "\n",
    "\n",
    "def spearman_r(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate Spearman's r\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Spearman's r\n",
    "    \"\"\"\n",
    "    return pearson_r(np.argsort(mod), np.argsort(obs))\n",
    "\n",
    "\n",
    "def kge(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the Kling-Gupta Efficiency (KGE)\n",
    "    (https://www.sciencedirect.com/science/article/pii/S0022169409004843)\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Kling-Gupta Efficiency\n",
    "    \"\"\"\n",
    "    r = pearson_r(obs, mod)\n",
    "    alpha = sd_ratio(obs, mod)\n",
    "    beta = np.sum(mod) / np.sum(obs)\n",
    "    return 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n",
    "\n",
    "\n",
    "def sd_ratio(obs, mod):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation ratio of the model predictions and observations\n",
    "    \n",
    "    Args:\n",
    "        obs: numpy array of observed values\n",
    "        mod: numpy array of modeled values\n",
    "    Returns:\n",
    "        Standard deviation ratio   \n",
    "    \"\"\"\n",
    "    return np.std(mod) / np.std(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b833ef3-e284-48db-be43-eb5d96840ed9",
   "metadata": {},
   "source": [
    " ## 1.3 Define benchmark function\n",
    "For each streamgage, we will compute a series of performance metrics, and the results from streamgage will be appended into a single dataframe, with one row per gage and one column for metric. To paralleize this task, we create the helper function `compute_benchmark()`, which computes the benchmark for a particular streamgage. In parallel, each worker in the cluster is assigned a gage, loads the data for that gage, computes the benchmark, and all the results are gathered. In this example, the `compute_benchmark()` funciton converts the data to a `pandas.Series`, resamples the model and observation data to the same timeseries, then computes a series of metrics from the resampled data. Each metric is stored as an entry in another `pandas.Series` named scores, which is returned by `compute_benchmark()` upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a39bd-c0fe-417a-be73-10299e7bac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_benchmark(gage_id, observations, predictions):\n",
    "    obs1 = observations.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "    mod1 = predictions.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() # Resampling could be done in preanalysis\n",
    "    # make sure the indices match\n",
    "    obs1.index = obs1.index.to_period('D')\n",
    "    mod1.index = mod1.index.to_period('D')\n",
    "\n",
    "\n",
    "    # merge obs and predictions and drop nans.\n",
    "    df = pd.merge(obs1, mod1, left_index=True, right_index=True).dropna(how='any')\n",
    "    obs1 = df['observed']\n",
    "    mod1 = df['predicted']\n",
    "    \n",
    "    # compute log flow for use in log NSE\n",
    "    threshold = 0.01\n",
    "    log_obs = np.log(obs1.where(obs1 > threshold, threshold))\n",
    "    log_model = np.log(mod1.where(mod1 > threshold, threshold))\n",
    "    \n",
    "    scores = pd.Series(dtype='float')\n",
    "    scores['nse'] = nse(obs1, mod1)\n",
    "    scores['log_nse'] = nse(log_obs, log_model)\n",
    "    scores['kge'] = kge(obs1, mod1)\n",
    "    \n",
    "    scores['pbias'] = pbias(obs1, mod1)\n",
    "    scores['pearson_r'] = pearson_r(obs1, mod1)\n",
    "    scores['spearman_r'] = spearman_r(obs1, mod1)\n",
    "    scores['sd_ratio'] = sd_ratio(obs1, mod1)\n",
    "    \n",
    "    # compute high flow and low flow bias\n",
    "    high_percentile = 98\n",
    "    low_percentile = 30\n",
    "    \n",
    "    scores['pbias_q' + str(high_percentile)] = pbias_percentile(obs1, mod1, high_percentile, np.greater)\n",
    "    scores['pbias_q' + str(low_percentile)] = pbias_percentile(obs1, mod1, high_percentile, np.less_equal)\n",
    "    scores.name = gage_id\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9c760-d1eb-45f6-b191-ca13452515dd",
   "metadata": {},
   "source": [
    "Run `compute_metrics()` and verify the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9654f30-6839-48e1-8062-5817868766d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, obs = load_streamflow_data(resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc67d14-ada2-444a-a6e7-09d973a1656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run for a single site using 1 core\n",
    "gage_id = 'USGS-01030350'\n",
    "compute_benchmark(gage_id, obs, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c10666-9dc3-44e7-b6e7-08e63a43545a",
   "metadata": {},
   "source": [
    "# 2. Compute benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63c23c-2cd8-42c2-8125-595567b26e66",
   "metadata": {},
   "source": [
    "We will define one final function, that wraps `compute_benchmark()` in a `try` statement. That way, if an error occurs at a particular streamgage, the other streamgages will be unaffected. \n",
    "\n",
    "WARNING: While developing your code, we recommend against sequestering errors inside a `try`, because error messages are extremely useful when debugging code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2e8df-dfab-4af6-873d-805c29dfe026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_compute_benchmark(gage_id):\n",
    "    \"\"\"Wrapper function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return compute_benchmark(gage_id, obs, mod)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e5bd6-2065-4980-9bb2-4e899c94b820",
   "metadata": {},
   "source": [
    "## 2.0 Setup cluster\n",
    "Using our helper function, we can setup our analysis in two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c53cc2-5369-44f5-b101-200e4a5c3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = configure_cluster(resource)\n",
    "mod, obs = load_streamflow_data(resource)\n",
    "\n",
    "#TODO could scatter here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d168f9-4087-4567-8995-6111cca0c9b0",
   "metadata": {},
   "source": [
    "## 2.1 Distribute with Dask bag\n",
    "Now to parallize, we create a Dask bag from the list `benchmark_gages`, and pass `try_compute_benchmark` to each element in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b820f-c1c0-4a95-a486-4e0ebdbc7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "b = db.from_sequence(benchmark_gages)\n",
    "b = b.map(try_compute_benchmark)\n",
    "results = b.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839c494-65ab-4ed7-8c68-e548d9e1cd22",
   "metadata": {},
   "source": [
    "Finally, concatenate the results from each gage into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43087ac2-278c-4c14-8ba5-811b941f0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [i for i in results if i is not None] # Drop entries where compute_metrics failed\n",
    "\n",
    "df_results = pd.concat(results, axis=1)\n",
    "df_results = df_results.T\n",
    "df_results.index.name = 'site_no'\n",
    "#df_results.index = df_results.index.astype('<U15')\n",
    "ds_results = df_results.to_xarray()\n",
    "ds_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad992e5-b1d6-4a78-8c9f-88f616124169",
   "metadata": {},
   "source": [
    "And save the results to disk. First as a csv, which will be uploaded to Science Base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d6984-2026-4321-8944-9f416d929513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_results.to_dataframe().to_csv('nwm_v2.1_streamflow_benchmark.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc15622-ef6e-41d1-8b26-f389b429f5dc",
   "metadata": {},
   "source": [
    "Then as a NetCDF, which we will later use for visualization. Let's add latitude and longitude from the benchmark data before writing the results to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e90005-11a6-4804-b828-0c6f96b360bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_results.merge(benchmark_ds, join='inner').to_netcdf('nwm_v2.1_streamflow_benchmark.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862ab3e-c624-462f-87ac-83f6cbdfd82d",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
